"""
Modul pentru analiza avansatÄƒ Ã®n 3 etape (Map-Reduce) cu LLM worker.
"""

import logging
import json
import os
import time
import uuid
from sqlmodel import Session, text
from typing import Dict, Any, List, Tuple, Optional
from ..settings_manager import settings_manager

logger = logging.getLogger(__name__)

class TwoRoundLLMAnalyzer:
    """
    OrchestreazÄƒ analiza avansatÄƒ (Map-Reduce) cu LLM worker.
    DeÈ™i numele este TwoRoundLLMAnalyzer (pentru compatibilitate), intern foloseÈ™te o arhitecturÄƒ Ã®n 3 etape:
    Phase 1: Discovery & Planning (Smart Projection)
    Phase 2: Batch Execution (Map)
    Phase 3: Final Synthesis (Reduce)
    """

    def __init__(self, session: Session):
        self.session = session
        self.plans_dir = "analyzer_plans"  # Directory for saving plans
        os.makedirs(self.plans_dir, exist_ok=True)

    async def analyze(self, user_query: str) -> Dict[str, Any]:
        """
        Wrapper principal care executÄƒ secvenÈ›ial cei 3 paÈ™i ai analizei.
        PÄƒstrat pentru compatibilitate cu apelurile existente.
        """
        try:
            logger.info(f"--- START 3-PHASE ANALYSIS: {user_query[:50]}... ---")

            # PHASE 1: Discovery & Planning
            plan_result = await self.create_plan(user_query)
            if not plan_result['success']:
                return plan_result

            plan_id = plan_result['plan_id']
            total_chunks = plan_result['total_chunks']
            logger.info(f"Plan {plan_id} creat. Se executÄƒ {total_chunks} chunk-uri...")

            # PHASE 2: Batch Execution (Map)
            # ExecutÄƒm chunk-urile secvenÈ›ial (se poate paralela Ã®n viitor)
            for i in range(total_chunks):
                chunk_res = await self.execute_chunk(plan_id, i)
                if not chunk_res['success']:
                    logger.error(f"Chunk {i} failed: {chunk_res.get('error')}")
                    # ContinuÄƒm execuÈ›ia celorlalte chunk-uri, faza 3 va gestiona datele lipsÄƒ

            # PHASE 3: Final Synthesis (Reduce)
            logger.info(f"Toate chunk-urile procesate. Se Ã®ncepe sinteza...")
            final_result = await self.synthesize_results(plan_id)

            return final_result

        except Exception as e:
            logger.error(f"[ANALYZER] Eroare criticÄƒ Ã®n procesul complet: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e)
            }

    async def create_plan(self, user_query: str) -> Dict[str, Any]:
        """
        PHASE 1: Discovery & Planning
        AnalizeazÄƒ cererea È™i creeazÄƒ un plan de execuÈ›ie optimizat.
        """
        try:
            logger.info(f"--- START PHASE 1: DISCOVERY & PLANNING for: {user_query[:50]}... ---")

            # 1. Generare strategie (SQL + Coloane)
            strategy = await self._generate_discovery_strategy(user_query)

            # 2. ExecuÈ›ie query-uri de descoperire (COUNT + ID_LIST)
            total_cases, all_ids = self._execute_discovery_queries(strategy)

            if total_cases == 0:
                return {
                    'success': False,
                    'error': 'Nu s-au gÄƒsit date relevante pentru aceastÄƒ interogare.'
                }

            # 3. Calculare Chunks
            # EstimÄƒm mÄƒrimea unui caz pe baza numÄƒrului de coloane selectate
            # Un caz full are ~2000 tokens. DacÄƒ selectÄƒm doar 3-4 coloane, avem ~200-300 tokens.
            # 30k tokens limitÄƒ / 300 tokens = ~100 cazuri per chunk.
            # Fiind conservatori, folosim 50 cazuri per chunk.
            chunk_size = 50
            chunks = [all_ids[i:i + chunk_size] for i in range(0, len(all_ids), chunk_size)]

            # 4. Generare Plan Object
            plan_id = str(uuid.uuid4())
            plan = {
                "plan_id": plan_id,
                "user_query": user_query,
                "strategy": strategy, # ConÈ›ine coloanele selectate
                "total_cases": total_cases,
                "total_chunks": len(chunks),
                "chunk_size": chunk_size,
                "chunks": chunks, # Lista de liste de ID-uri
                "created_at": time.time(),
                "status": "created"
            }

            # 5. Salvare Plan
            self._save_plan(plan)

            # 6. Preview (OpÈ›ional - primele 3 cazuri pentru UI/Log)
            preview_ids = all_ids[:3]
            preview_data = self._fetch_chunk_data(preview_ids, strategy['selected_columns'])

            logger.info(f"[PHASE 1] Plan creat: {plan_id}. Total cazuri: {total_cases}. Chunks: {len(chunks)}.")

            return {
                'success': True,
                'plan_id': plan_id,
                'total_cases': total_cases,
                'total_chunks': len(chunks),
                'estimated_time_seconds': len(chunks) * 5, # Estimare grosierÄƒ
                'preview_data': preview_data,
                'strategy_summary': strategy.get('rationale', 'Strategie generatÄƒ automat.')
            }

        except Exception as e:
            logger.error(f"[PHASE 1] Eroare criticÄƒ: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e)
            }

    async def execute_chunk(self, plan_id: str, chunk_index: int) -> Dict[str, Any]:
        """
        PHASE 2: Batch Execution (Worker)
        ExecutÄƒ analiza pentru un singur chunk.
        """
        try:
            # 1. ÃŽncÄƒrcare Plan
            plan_path = os.path.join(self.plans_dir, f"{plan_id}.json")
            if not os.path.exists(plan_path):
                raise FileNotFoundError(f"Planul {plan_id} nu existÄƒ.")

            with open(plan_path, 'r', encoding='utf-8') as f:
                plan = json.load(f)

            chunks = plan['chunks']
            if chunk_index < 0 or chunk_index >= len(chunks):
                raise IndexError(f"Chunk index {chunk_index} invalid. Total chunks: {len(chunks)}")

            chunk_ids = chunks[chunk_index]
            selected_columns = plan['strategy']['selected_columns']
            user_query = plan['user_query']

            logger.info(f"[PHASE 2] Executing Chunk {chunk_index + 1}/{len(chunks)} for Plan {plan_id}. IDs: {len(chunk_ids)}")

            # 2. Smart Fetch
            chunk_data = self._fetch_chunk_data(chunk_ids, selected_columns)

            # 2.1. Validare È™i Truncare (Safety Net)
            # Chiar dacÄƒ avem chunk-uri mici, textele pot fi enorme.
            truncated_data, metadata = self._validate_and_truncate_data(chunk_data, user_query, max_chars=30000)

            if metadata['truncated']:
                logger.warning(f"[PHASE 2] Chunk {chunk_index} truncated: {metadata['cases_included_in_prompt']}/{len(chunk_data)} cases included.")

            # 3. AnalizÄƒ LLM (Map)
            from ..lib.network_file_saver import NetworkFileSaver

            prompt = self._build_chunk_analysis_prompt(user_query, truncated_data, chunk_index, len(chunks))

            # LogicÄƒ de reÈ›ea
            retea_host = settings_manager.get_value('setari_retea', 'retea_host', '')
            retea_folder = settings_manager.get_value('setari_retea', 'retea_folder_partajat', '')

            success, message, saved_path = NetworkFileSaver.save_to_network(
                content=prompt,
                host=retea_host,
                shared_folder=retea_folder,
                subfolder=''
            )

            if not success:
                raise RuntimeError(f"Eroare salvare prompt Chunk {chunk_index}: {message}")

            poll_success, poll_content, response_path = await NetworkFileSaver.poll_for_response(
                saved_path=saved_path,
                timeout_seconds=600,
                poll_interval=5
            )

            if not poll_success:
                raise RuntimeError(f"Timeout Chunk {chunk_index}: {poll_content}")

            # Parsare rezultat
            try:
                chunk_result = self._parse_json_response(poll_content)
                NetworkFileSaver.delete_response_file(response_path)
            except Exception as e:
                logger.error(f"Eroare parsare rÄƒspuns Chunk {chunk_index}: {e}")
                # Nu crÄƒpÄƒm tot procesul, returnÄƒm eroare pentru acest chunk
                return {
                    'success': False,
                    'chunk_index': chunk_index,
                    'error': str(e),
                    'raw_response': poll_content[:1000]
                }

            # 4. Salvare Rezultat Chunk
            result_file = os.path.join(self.plans_dir, f"{plan_id}_chunk_{chunk_index}.json")
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(chunk_result, f, indent=2, ensure_ascii=False)

            return {
                'success': True,
                'chunk_index': chunk_index,
                'cases_analyzed': len(chunk_data),
                'result_summary': chunk_result.get('summary', 'N/A')
            }

        except Exception as e:
            logger.error(f"[PHASE 2] Eroare Chunk {chunk_index}: {e}", exc_info=True)
            return {
                'success': False,
                'chunk_index': chunk_index,
                'error': str(e)
            }

    async def synthesize_results(self, plan_id: str) -> Dict[str, Any]:
        """
        PHASE 3: Final Synthesis (Analyst)
        AgregÄƒ rezultatele È™i genereazÄƒ rÄƒspunsul final.
        """
        try:
            # 1. ÃŽncÄƒrcare Plan
            plan_path = os.path.join(self.plans_dir, f"{plan_id}.json")
            if not os.path.exists(plan_path):
                raise FileNotFoundError(f"Planul {plan_id} nu existÄƒ.")

            with open(plan_path, 'r', encoding='utf-8') as f:
                plan = json.load(f)

            user_query = plan['user_query']
            total_chunks = plan['total_chunks']

            # 2. ÃŽncÄƒrcare Rezultate Chunks
            aggregated_data = []
            missing_chunks = []

            for i in range(total_chunks):
                chunk_file = os.path.join(self.plans_dir, f"{plan_id}_chunk_{i}.json")
                if os.path.exists(chunk_file):
                    with open(chunk_file, 'r', encoding='utf-8') as f:
                        chunk_res = json.load(f)
                        aggregated_data.append(chunk_res)
                else:
                    missing_chunks.append(i)

            if not aggregated_data:
                return {
                    'success': False,
                    'error': 'Nu existÄƒ rezultate de la chunks pentru a fi agregate.'
                }

            logger.info(f"[PHASE 3] Synthesizing results from {len(aggregated_data)} chunks. Missing: {len(missing_chunks)}")

            # 3. SintezÄƒ LLM (Reduce)
            from ..lib.network_file_saver import NetworkFileSaver

            prompt = self._build_synthesis_prompt(user_query, aggregated_data, missing_chunks)

            # LogicÄƒ de reÈ›ea
            retea_host = settings_manager.get_value('setari_retea', 'retea_host', '')
            retea_folder = settings_manager.get_value('setari_retea', 'retea_folder_partajat', '')

            success, message, saved_path = NetworkFileSaver.save_to_network(
                content=prompt,
                host=retea_host,
                shared_folder=retea_folder,
                subfolder=''
            )

            if not success:
                raise RuntimeError(f"Eroare salvare prompt Synthesis: {message}")

            poll_success, poll_content, response_path = await NetworkFileSaver.poll_for_response(
                saved_path=saved_path,
                timeout_seconds=600,
                poll_interval=5
            )

            if not poll_success:
                raise RuntimeError(f"Timeout Synthesis: {poll_content}")

            # Parsare rezultat final
            try:
                final_result = self._parse_json_response(poll_content)
                NetworkFileSaver.delete_response_file(response_path)
            except Exception as e:
                logger.error(f"Eroare parsare rÄƒspuns Synthesis: {e}")
                raise ValueError(f"LLM a returnat un rÄƒspuns invalid Ã®n Phase 3: {e}")

            # AdÄƒugÄƒm metadate despre proces
            final_result['process_metadata'] = {
                'plan_id': plan_id,
                'total_cases': plan['total_cases'],
                'chunks_processed': len(aggregated_data),
                'chunks_missing': len(missing_chunks)
            }

            return final_result

        except Exception as e:
            logger.error(f"[PHASE 3] Eroare Synthesis: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e)
            }

    # =================================================================================================
    # HELPER METHODS - PROMPTS & EXECUTION
    # =================================================================================================

    def _build_synthesis_prompt(self, user_query: str, aggregated_data: List[Dict], missing_chunks: List[int]) -> str:
        # MinimizÄƒm datele trimise la sintezÄƒ pentru a nu depÄƒÈ™i contextul
        # Trimitem doar 'extracted_data' È™i 'partial_stats' din fiecare chunk

        clean_aggregation = []
        for chunk in aggregated_data:
            clean_aggregation.append({
                "chunk_index": chunk.get("chunk_index"),
                "extracted_data": chunk.get("extracted_data"),
                "partial_stats": chunk.get("partial_stats")
            })

        data_json = json.dumps(clean_aggregation, indent=2, ensure_ascii=False)

        missing_info = ""
        if missing_chunks:
            missing_info = f"\nâš ï¸ ATENÈšIE: Lipsesc datele din chunks: {missing_chunks}. Rezultatul poate fi incomplet."

        return f"""===================================================================================
ðŸ”¬ PHASE 3: FINAL SYNTHESIS (REDUCE)
===================================================================================
Tu eÈ™ti Analistul È˜ef. Ai primit rapoarte parÈ›iale de la mai mulÈ›i workeri (chunks).
Trebuie sÄƒ agregezi aceste date È™i sÄƒ formulezi RÄ‚SPUNSUL FINAL pentru utilizator.

TASK UTILIZATOR: "{user_query}"
{missing_info}

=================================================================================== ðŸ“¦ REZULTATE AGREGATE (CHUNKS)
{data_json}

=================================================================================== ðŸŽ¯ MISIUNEA TA
1. AgregÄƒ datele numerice (calculeazÄƒ medii ponderate, sume totale etc.).
2. IdentificÄƒ tendinÈ›ele calitative din datele extrase.
3. FormuleazÄƒ un rÄƒspuns final clar, profesional È™i bazat STRICT pe date.

=================================================================================== ðŸ“¤ FORMAT RÄ‚SPUNS (JSON)
{{
  "results": {{
      "total_analyzed": 150,
      "final_stats": {{ "mean": 5.5, "median": 5, "unit": "ani" }},
      "distribution": {{ "1-3 ani": 10, "3-5 ani": 20 }}
  }},
  "interpretation": "Analiza a 150 de cazuri aratÄƒ cÄƒ media pedepselor este de 5.5 ani...",
  "charts": [
      {{ "type": "bar", "title": "DistribuÈ›ie", "data": ... }}
  ]
}}

RÄ‚SPUNDE DOAR CU JSON:
"""

    def _build_chunk_analysis_prompt(self, user_query: str, chunk_data: List[Dict], chunk_index: int, total_chunks: int) -> str:
        data_json = json.dumps(chunk_data, indent=2, ensure_ascii=False)
        return f"""===================================================================================
ðŸ”¬ PHASE 2: BATCH EXECUTION (CHUNK {chunk_index + 1}/{total_chunks})
===================================================================================
Tu eÈ™ti un Analist de Date (Worker). Analizezi un mic lot de date (Chunk) ca parte a unui proces mai mare.

TASK UTILIZATOR: "{user_query}"

=================================================================================== ðŸ“¦ DATELE TALE (CHUNK)
{data_json}

=================================================================================== ðŸŽ¯ MISIUNEA TA
AnalizeazÄƒ ACEST set de date È™i extrage informaÈ›iile relevante pentru task.
NU Ã®ncerca sÄƒ rÄƒspunzi final la Ã®ntrebare! Doar extrage datele brute sau statistici parÈ›iale.

1. **Extragere valori numerice**:
   - CautÄƒ pattern-uri: "X ani", "X luni", "X lei".
   - DacÄƒ cÃ¢mpul principal e gol, cautÄƒ Ã®n celelalte cÃ¢mpuri selectate.

2. **SintezÄƒ parÈ›ialÄƒ**:
   - NumÄƒrÄƒ cazurile relevante din acest chunk.
   - CalculeazÄƒ sume/medii parÈ›iale dacÄƒ e posibil.

=================================================================================== ðŸ“¤ FORMAT RÄ‚SPUNS (JSON)
{{
  "chunk_index": {chunk_index},
  "analyzed_count": {len(chunk_data)},
  "extracted_data": [
      {{ "id": 123, "valoare": 5, "unitate": "ani", "context": "pedeapsa principala" }},
      {{ "id": 124, "valoare": null, "motiv": "nu s-a gasit in text" }}
  ],
  "partial_stats": {{
      "sum": 5,
      "count": 1,
      "min": 5,
      "max": 5
  }},
  "qualitative_notes": "Un caz relevant identificat."
}}

âš ï¸ REGULI:
- RÄƒspunde DOAR cu JSON.
- Nu inventa date.
- DacÄƒ nu gÄƒseÈ™ti nimic, returneazÄƒ liste goale.

RÄ‚SPUNDE DOAR CU JSON:
"""

    async def _generate_discovery_strategy(self, user_query: str) -> Dict[str, Any]:
        """
        FoloseÈ™te LLM pentru a genera SQL-ul de discovery È™i lista de coloane necesare.
        """
        from ..lib.network_file_saver import NetworkFileSaver

        prompt = self._build_discovery_prompt(user_query)

        # LogicÄƒ de reÈ›ea
        retea_host = settings_manager.get_value('setari_retea', 'retea_host', '')
        retea_folder = settings_manager.get_value('setari_retea', 'retea_folder_partajat', '')

        success, message, saved_path = NetworkFileSaver.save_to_network(
            content=prompt,
            host=retea_host,
            shared_folder=retea_folder,
            subfolder=''
        )

        if not success:
            raise RuntimeError(f"Eroare salvare prompt Discovery: {message}")

        poll_success, poll_content, response_path = await NetworkFileSaver.poll_for_response(
            saved_path=saved_path,
            timeout_seconds=600,
            poll_interval=5
        )

        if not poll_success:
            raise RuntimeError(f"Timeout Discovery: {poll_content}")

        # Parsare
        try:
            strategy = self._parse_json_response(poll_content)
            NetworkFileSaver.delete_response_file(response_path)
            return strategy
        except Exception as e:
            logger.error(f"Eroare parsare rÄƒspuns Discovery: {e}")
            raise ValueError(f"LLM a returnat un rÄƒspuns invalid Ã®n Phase 1: {e}")

    def _build_discovery_prompt(self, user_query: str) -> str:
        return f"""===================================================================================
ðŸ”¬ PHASE 1: DISCOVERY & PLANNING (SMART PROJECTION)
===================================================================================
Tu eÈ™ti Arhitectul Sistemului. Scopul tÄƒu este sÄƒ planifici execuÈ›ia eficientÄƒ pentru o analizÄƒ Big Data pe cazuri juridice.

TASK UTILIZATOR: "{user_query}"

=================================================================================== ðŸ“Š SCHEMA BAZEI DE DATE
Tabel: blocuri (id INTEGER PRIMARY KEY, obj JSONB)
CÃ¢mpuri JSONB disponibile:
1. numÄƒr_dosar (string) - Ex: "SentinÅ£a CivilÄƒ nr.93", "Decizie nr. 1405/2021"
2. tip_solutie (string) - Ex: "StabileÅŸte competenÅ£a", "Respinge apelul"
3. tip_cale_atac (string) - Ex: "DefinitivÄƒ", "Apel", "Recurs"
4. cereri_accesorii (string) - Ex: "cheltuieli de judecatÄƒ", "daune materiale"
5. tip_act_juridic (string) - Ex: "Cerere de chemare Ã®n judecatÄƒ", "ContestaÈ›ie la executare"
6. probele_retinute (string) - Ex: "Ã®nscrisuri", "DeclaraÈ›ii martori"
7. keywords (array/string) - Ex: ["Conflict de competenÈ›Äƒ"], ["executare silitÄƒ"]
8. titlu (string) - Titlul complet al deciziei
9. text_denumire_articol (string) - Titlul articolului pentru SEO
10. text_situatia_de_fapt (string) - Descriere detaliatÄƒ a faptelor (FOARTE MARE!)
11. text_ce_invatam (string) - Principii de drept È™i lecÈ›ii extrase
12. text_individualizare (string) - Elementele unice, pedepse (FOARTE IMPORTANT!)
13. text_doctrina (string) - ReferinÈ›e doctrinare
14. sursa (string) - Sursa datelor
15. obiect (string) - Ex: "conflict negativ de competenÅ£Äƒ", "omor"
16. materie (string) - Ex: "Codul Muncii", "Codul Penal"
17. articol_incident (string) - Articole de lege invocate
18. Rezumat_generat_de_AI_Cod (string) - Rezumat concis
19. analiza_judecator (string) - Analiza criticÄƒ
20. Considerentele (string) - Motivarea instanÈ›ei (FOARTE MARE!)
21. Dispozitivul (string) - Minuta deciziei
22. argumente_instanta (string) - Argumentele instanÈ›ei
23. solutia (string) - SoluÈ›ia pe scurt (poate include pedepse)
24. considerente_speta (string) - Motivare specificÄƒ
25. data_solutiei (string) - Data pronunÈ›Äƒrii (YYYY-MM-DD)

=================================================================================== ðŸŽ¯ MISIUNEA TA
1. GenereazÄƒ un SQL COUNT query pentru a vedea volumul total.
2. GenereazÄƒ un SQL ID_LIST query pentru a obÈ›ine toate ID-urile relevante.
3. IDENTIFICÄ‚ STRICT COLOANELE NECESARE din JSONB (Smart Projection).
   - NU selecta niciodatÄƒ 'obj' complet!
   - SelecteazÄƒ doar cÃ¢mpurile care rÄƒspund la Ã®ntrebare.

=================================================================================== ðŸ“š EXEMPLE DE STRATEGIE

Exemplu 1: "Care este durata medie a pedepselor pentru omor?"
{{
  "count_query": "SELECT COUNT(*) FROM blocuri WHERE obj->>'materie' ILIKE '%penal%' AND obj->>'obiect' ILIKE '%omor%'",
  "id_list_query": "SELECT id FROM blocuri WHERE obj->>'materie' ILIKE '%penal%' AND obj->>'obiect' ILIKE '%omor%'",
  "selected_columns": ["solutia", "text_individualizare", "obiect", "materie"],
  "rationale": "Am selectat 'solutia' È™i 'text_individualizare' pentru a extrage durata pedepselor. 'obiect' È™i 'materie' sunt pentru context."
}}

Exemplu 2: "EvoluÈ›ia amenzilor pentru furt Ã®n ultimii 5 ani"
{{
  "count_query": "SELECT COUNT(*) FROM blocuri WHERE obj->>'obiect' ILIKE '%furt%' AND obj->>'solutia' ~ '\\\\d+(\\\\.\\\\d+)?\\\\s*lei' AND obj->>'data_solutiei' >= '2020-01-01'",
  "id_list_query": "SELECT id FROM blocuri WHERE obj->>'obiect' ILIKE '%furt%' AND obj->>'solutia' ~ '\\\\d+(\\\\.\\\\d+)?\\\\s*lei' AND obj->>'data_solutiei' >= '2020-01-01'",
  "selected_columns": ["solutia", "data_solutiei", "obiect"],
  "rationale": "Am nevoie de 'solutia' pentru sume È™i 'data_solutiei' pentru evoluÈ›ia Ã®n timp."
}}

=================================================================================== ðŸ“¤ FORMAT RÄ‚SPUNS (JSON)
{{
  "count_query": "SELECT COUNT(*) FROM blocuri WHERE ...",
  "id_list_query": "SELECT id FROM blocuri WHERE ...",
  "selected_columns": ["col1", "col2"],
  "rationale": "Explicatie..."
}}

âš ï¸ REGULI:
- Queries trebuie sÄƒ fie PostgreSQL valid.
- id_list_query trebuie sÄƒ returneze DOAR coloana 'id'.
- selected_columns trebuie sÄƒ fie o listÄƒ de string-uri (chei din JSONB).
- Fii strict cu filtrele WHERE pentru a elimina zgomotul.
- FoloseÈ™te operatorul `->>` pentru a accesa cÃ¢mpuri JSONB ca text.
- Pentru array-uri (ex: keywords), foloseÈ™te `~` (regex) nu `ILIKE`.

RÄ‚SPUNDE DOAR CU JSON:
"""

    def _execute_discovery_queries(self, strategy: Dict[str, Any]) -> Tuple[int, List[int]]:
        """ExecutÄƒ query-urile generate pentru a obÈ›ine count È™i lista de ID-uri."""

        # Validare basic
        if "count_query" not in strategy or "id_list_query" not in strategy:
            raise ValueError("Strategia nu conÈ›ine query-urile necesare.")

        count_sql = strategy['count_query']
        ids_sql = strategy['id_list_query']

        # ExecuÈ›ie COUNT
        try:
            count_res = self.session.execute(text(count_sql)).scalar()
        except Exception as e:
            logger.error(f"Eroare execuÈ›ie COUNT query: {e}")
            raise ValueError(f"Query COUNT invalid: {e}")

        # ExecuÈ›ie ID LIST
        try:
            ids_res = self.session.execute(text(ids_sql)).scalars().all()
        except Exception as e:
            logger.error(f"Eroare execuÈ›ie ID_LIST query: {e}")
            raise ValueError(f"Query ID_LIST invalid: {e}")

        return count_res, list(ids_res)

    def _fetch_chunk_data(self, ids: List[int], columns: List[str]) -> List[Dict]:
        """
        Smart Fetch: Extrage doar coloanele specificate pentru o listÄƒ de ID-uri.
        ConstruieÈ™te dinamic query-ul SQL.
        """
        if not ids:
            return []

        # Construire SELECT dinamic
        # SELECT id, obj->>'col1' as col1, obj->>'col2' as col2 FROM blocuri WHERE id IN (...)

        select_parts = ["id"]
        for col in columns:
            # Sanitizare simplÄƒ pentru a preveni injecÈ›ii grosolane, deÈ™i coloanele vin din LLM
            clean_col = col.replace("'", "")
            select_parts.append(f"obj->>'{clean_col}' as \"{clean_col}\"")

        select_clause = ", ".join(select_parts)
        ids_str = ",".join(map(str, ids))

        sql = f"SELECT {select_clause} FROM blocuri WHERE id IN ({ids_str})"

        results = self.session.execute(text(sql)).mappings().all()
        return [dict(r) for r in results]

    def _save_plan(self, plan: Dict[str, Any]):
        """SalveazÄƒ planul pe disk."""
        file_path = os.path.join(self.plans_dir, f"{plan['plan_id']}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(plan, f, indent=2, ensure_ascii=False)

    def _identify_query_type(self, query: str) -> str:
        """IdentificÄƒ tipul query-ului bazat pe cuvinte cheie."""
        query_lower = query.lower()

        # Detectare durate/pedepse
        if any(word in query_lower for word in ['durata', 'pedeapsa', 'pedepse', 'ani', 'luni', 'condamnare', 'inchisoare', 'detentie']):
            return 'durate'

        # Detectare amenzi
        elif any(word in query_lower for word in ['amenda', 'amendÄƒ', 'lei', 'suma', 'bani']):
            return 'amenzi'

        # Detectare tendinÈ›e temporale
        elif any(word in query_lower for word in ['evolutie', 'evoluÈ›ie', 'tendinta', 'tendinÈ›Äƒ', 'timp', 'crestere', 'scadere', 'perioada']):
            return 'tendinte'

        # Detectare motive/considerente
        elif any(word in query_lower for word in ['motiv', 'considerent', 'argumentare', 'justificare', 'rationament']):
            return 'motive'

        # Default: general
        else:
            return 'general'

    def _extract_relevant_fields(self, user_query: str, filtered_data: List[Dict]) -> List[Dict]:
        """Extrage doar cÃ¢mpurile relevante pentru query, reducÃ¢nd dimensiunea datelor."""

        # IdentificÄƒ tipul query-ului
        query_type = self._identify_query_type(user_query)

        logger.info(f"[EXTRAGERE] Query type identificat: {query_type}")

        # Mapping cÃ¢mpuri relevante pentru fiecare tip de query
        field_mappings = {
            'durate': ['id', 'obiect', 'materie', 'text_individualizare', 'individualizare', 'solutia', 'solutie', 'data_solutiei'],
            'amenzi': ['id', 'obiect', 'materie', 'solutia', 'solutie', 'considerente_speta', 'considerente'],
            'tendinte': ['id', 'obiect', 'materie', 'solutia', 'solutie', 'data_solutiei'],
            'motive': ['id', 'obiect', 'materie', 'considerente_speta', 'considerente', 'argumente_instanta', 'solutia', 'solutie'],
            'general': ['id', 'obiect', 'materie', 'solutia', 'solutie', 'text_individualizare', 'individualizare', 'considerente_speta']
        }

        relevant_fields = field_mappings.get(query_type, field_mappings['general'])

        # Extragere cÃ¢mpuri relevante
        result = []
        for case in filtered_data:
            filtered_case = {}
            for field in relevant_fields:
                if field in case:
                    value = case[field]
                    # TruncÄƒm textele foarte lungi (> 2000 chars) pentru a economisi spaÈ›iu
                    if isinstance(value, str) and len(value) > 2000:
                        filtered_case[field] = value[:2000] + "...[truncat]"
                    else:
                        filtered_case[field] = value

            # Include Ã®ntotdeauna ID-ul
            if 'id' not in filtered_case and 'id' in case:
                filtered_case['id'] = case['id']

            result.append(filtered_case)

        logger.info(f"[EXTRAGERE] Redus de la {len(filtered_data)} cazuri cu toate cÃ¢mpurile la {len(result)} cazuri cu cÃ¢mpuri relevante")

        return result

    def _validate_and_truncate_data(
        self,
        filtered_data: List[Dict],
        user_query: str,
        max_chars: int = 30000
    ) -> Tuple[List[Dict], Dict[str, Any]]:
        """
        ValideazÄƒ È™i truncÄƒ datele pentru a nu depÄƒÈ™i max_chars.
        Returns: Tuple[truncated_data, metadata]
        """

        # Construim un prompt gol pentru a estima dimensiunea de bazÄƒ
        base_prompt = f"""===================================================================================
ðŸ”¬ ROUND 2: ANALIZA DATELOR FILTRATE
Tu eÈ™ti un Data Scientist È™i Analist Juridic Senior.

TASK-UL ORIGINAL AL UTILIZATORULUI: {user_query}

CONTEXT: ÃŽn ROUND 1, am extras cazuri relevante din baza de date.

===================================================================================
ðŸ“¦ DATELE EXTRASE

===================================================================================
ðŸŽ¯ MISIUNEA TA (ROUND 2)
AnalizeazÄƒ datele de mai sus È™i genereazÄƒ:
1. Statistici descriptive (medie, medianÄƒ, etc.)
2. TendinÈ›e (evoluÈ›ie Ã®n timp)
3. CorelaÈ›ii (dacÄƒ e relevant)
4. Interpretare Ã®n limbaj natural (concluzii clare)

===================================================================================
ðŸ“¤ FORMAT RÄ‚SPUNS - JSON OBLIGATORIU
{{
  "results": {{
    "total_cases_analyzed": 87,
    "mean_sentence_years": 15.3
  }},
  "interpretation": "Analiza relevÄƒ...",
  "charts": []
}}

RÄ‚SPUNDE DOAR CU JSON:
"""

        base_size = len(base_prompt)

        # SpaÈ›iu disponibil pentru date (cu buffer de siguranÈ›Äƒ de 2000 chars)
        available_space = max_chars - base_size - 2000

        if available_space <= 0:
            logger.warning(f"[VALIDARE] Base prompt prea mare: {base_size} chars. ForÈ›Äƒm spaÈ›iu minim.")
            available_space = 5000  # Minimum absolut pentru date

        logger.info(f"[VALIDARE] SpaÈ›iu disponibil pentru date: {available_space} caractere")

        # Procesare date cu truncare progresivÄƒ
        truncated_data = []
        current_size = 0
        cases_included = 0

        for case in filtered_data:
            # Serializare caz individual
            case_json = json.dumps(case, ensure_ascii=False, separators=(',', ':'))  # Compact JSON
            case_size = len(case_json)

            # VerificÄƒm dacÄƒ mai avem spaÈ›iu
            if current_size + case_size + 10 <= available_space:  # +10 pentru separatori
                truncated_data.append(case)
                current_size += case_size + 10
                cases_included += 1
            else:
                # Nu mai avem spaÈ›iu, oprim
                logger.info(f"[VALIDARE] Truncare la {cases_included} cazuri pentru a respecta limita")
                break

        # CalculÄƒm dimensiunea finalÄƒ estimatÄƒ
        final_data_json = json.dumps(truncated_data, indent=2, ensure_ascii=False)
        final_data_size = len(final_data_json)
        estimated_total = base_size + final_data_size

        metadata = {
            'total_cases_filtered': len(filtered_data),
            'cases_included_in_prompt': cases_included,
            'base_prompt_size': base_size,
            'data_size': final_data_size,
            'estimated_prompt_size': estimated_total,
            'truncated': cases_included < len(filtered_data),
            'available_space': available_space,
            'max_chars_limit': max_chars
        }

        # Log important pentru debugging
        logger.info(f"[VALIDARE] âœ“ Prompt Round 2 validat:")
        logger.info(f"  - Cazuri incluse: {cases_included}/{len(filtered_data)}")
        logger.info(f"  - Dimensiune estimatÄƒ: {estimated_total:,} / {max_chars:,} caractere")
        logger.info(f"  - SpaÈ›iu rÄƒmas: {max_chars - estimated_total:,} caractere")

        if estimated_total > max_chars:
            logger.warning(f"[VALIDARE] âš ï¸ ATENÈšIE: Prompt estimat ({estimated_total}) depÄƒÈ™eÈ™te limita ({max_chars})!")

        return truncated_data, metadata

    def _parse_json_response(self, content: str) -> Dict[str, Any]:
        """ParseazÄƒ rÄƒspunsul JSON de la LLM, curÄƒÈ›Ã¢nd eventualele markdown fences."""
        cleaned = content.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]

        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]

        # Eliminare caractere invizibile/spaÈ›ii
        cleaned = cleaned.strip()

        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
             # Fallback: Ã®ncercÄƒm sÄƒ gÄƒsim primul { È™i ultimul }
            start = cleaned.find("{")
            end = cleaned.rfind("}")
            if start != -1 and end != -1:
                json_str = cleaned[start:end+1]
                return json.loads(json_str)
            raise
